{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'sample_submission.csv', 'train']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import glob\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import time\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import gc\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Activation,Dropout,BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data = '../input/'\n",
    "train_files = sorted(glob.glob(data + 'train/*/*'))\n",
    "# test_path = sorted(glob.glob(data + 'test/*'))\n",
    "\n",
    "labels = ['HTC-1-M7', 'iPhone-4s', 'iPhone-6', 'LG-Nexus-5x', 'Motorola-Droid-Maxx', \n",
    "         'Motorola-Nexus-6', 'Motorola-X', 'Samsung-Galaxy-Note3', 'Samsung-Galaxy-S4',\n",
    "         'Sony-NEX-7'] \n",
    "All_labels = [labels.index(file.split('/')[-2]) for file in train_files] # TO BE CHANGED!!!!!!!\n",
    "\n",
    "x_train_files, x_test_files, y_train_files, y_test_files = train_test_split(train_files,All_labels,test_size = 0.2, shuffle = True,random_state = 42)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train_files, len(labels)) # one hot vector\n",
    "y_test = keras.utils.to_categorical(y_test_files, len(labels)) # one hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "a55df51c3f6cc910977567a4ea61581d8981a774"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b838241c093b4df0819a785f239a912e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dim = 256\n",
    "\n",
    "kernel_filter = 1/12. * np.array([\\\n",
    "            [-1,  2,  -2,  2, -1],  \\\n",
    "            [ 2, -6,   8, -6,  2],  \\\n",
    "            [-2,  8, -12,  8, -2],  \\\n",
    "            [ 2, -6,   8, -6,  2],  \\\n",
    "            [-1,  2,  -2,  2, -1]])\n",
    "\n",
    "\n",
    "train_imgs = []\n",
    "for file in tqdm_notebook(x_train_files):\n",
    "    img = cv2.imread(file)\n",
    "    center_x = img.shape[0]//2\n",
    "    center_y = img.shape[1]//2 \n",
    "\n",
    "    cropped_img = img[center_x-dim//2:center_x+dim//2, center_y-dim//2:center_y+dim//2, : ]\n",
    "    \n",
    "    cropped_img = cv2.filter2D(cropped_img.astype(np.float32),-1,kernel_filter)\n",
    "    \n",
    "    train_imgs.append(cropped_img)\n",
    "    del file,img,center_x,center_y,cropped_img\n",
    "    gc.collect()\n",
    "\n",
    "train_imgs = np.array(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "8a5a77e9406985c267568b761e6fddb53391cfc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "eb194f5a6df02abbae3a4180cbbb55bcca4e4b06"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip = True)\n",
    "\n",
    "datagen.fit(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "9890c5e4a394ee603554a46ae436e415259031cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 127, 127, 64)      1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 127, 127, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 127, 127, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 63, 63, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 63, 63, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 63, 63, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 61, 61, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 61, 61, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 61, 61, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 28800)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               7373056   \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 4096)              1052672   \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                40970     \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 8,524,522\n",
      "Trainable params: 8,524,202\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(3, 3),strides=(2, 2), input_shape=(256, 256, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3),strides=(2, 2) ))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7342ae76fb08b4ed9980cd52b50e9783e1dc952d"
   },
   "outputs": [],
   "source": [
    "model.fit_generator(datagen.flow(train_imgs, y_train, batch_size=32),steps_per_epoch= 2200 / 32, epochs=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80f0b98ab5f65265d473088e79935aa48ee61007"
   },
   "outputs": [],
   "source": [
    "# del train_imgs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "fd0a98183e00a65dc91932c99749ead42c323130"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f8e3040615416a904976102066b298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=550), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dim = 256\n",
    "\n",
    "kernel_filter = 1/12. * np.array([\\\n",
    "            [-1,  2,  -2,  2, -1],  \\\n",
    "            [ 2, -6,   8, -6,  2],  \\\n",
    "            [-2,  8, -12,  8, -2],  \\\n",
    "            [ 2, -6,   8, -6,  2],  \\\n",
    "            [-1,  2,  -2,  2, -1]])\n",
    "\n",
    "\n",
    "test_imgs = []\n",
    "for file in tqdm_notebook(x_test_files):\n",
    "    img = cv2.imread(file)\n",
    "    center_x = img.shape[0]//2\n",
    "    center_y = img.shape[1]//2 \n",
    "    cropped_img = img[center_x-dim//2:center_x+dim//2, center_y-dim//2:center_y+dim//2, : ]\n",
    "    \n",
    "    cropped_img = cv2.filter2D(cropped_img.astype(np.float32),-1,kernel_filter)    \n",
    "    test_imgs.append(cropped_img)\n",
    "    del file,img,center_x,center_y,cropped_img\n",
    "    gc.collect()\n",
    "\n",
    "test_imgs = np.array(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "80249ca0bdd8227ba16aec0bd435976b0fe87037"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/550 [==============================] - 1s 2ms/step\n",
      "Test loss: 0.6678022979606282\n",
      "Test accuracy: 0.7890909088741649\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_imgs,y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "79a92583e98d348fdbe6a686f0b475d348bea67e"
   },
   "outputs": [],
   "source": [
    "del test_imgs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f856d9355a071016b32b1be1d449be25b6349ca5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
